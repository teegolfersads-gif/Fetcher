name: Fetch US News (Clean Articles Only)

on:
  schedule:
    - cron: "*/30 * * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4

      - name: Fetch US news
        run: |
          python - <<'PY'
          import json
          import re
          import urllib.request
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlparse
          from pathlib import Path
          from datetime import datetime, timezone

          HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
          }

          SOURCES = [
              {"url": "https://apnews.com/us-news", "domain": "apnews.com"},
              {"url": "https://apnews.com/politics", "domain": "apnews.com"},
              {"url": "https://apnews.com/world-news", "domain": "apnews.com"},

              {"url": "https://www.npr.org/sections/national/", "domain": "www.npr.org"},
              {"url": "https://www.pbs.org/newshour/", "domain": "www.pbs.org"},
              {"url": "https://news.yahoo.com/us/", "domain": "news.yahoo.com"},
              {"url": "https://www.huffpost.com/news/politics", "domain": "www.huffpost.com"},

              {"url": "https://www.vox.com/politics", "domain": "www.vox.com"},

              {"url": "https://www.politico.com/white-house-news-updates-analysis", "domain": "www.politico.com"},
              {"url": "https://www.politico.com/news/california", "domain": "www.politico.com"},
              {"url": "https://www.politico.com/news/new-york", "domain": "www.politico.com"},

              {"url": "https://www.foxnews.com/us", "domain": "www.foxnews.com"},
              {"url": "https://www.foxnews.com/politics", "domain": "www.foxnews.com"},

              {"url": "https://www.nbcnews.com/politics", "domain": "www.nbcnews.com"},
              {"url": "https://www.nbcnews.com/us-news", "domain": "www.nbcnews.com"},
          ]

          BAD_KEYWORDS = [
              "/tag/", "/tags/", "/topic/", "/topics/", "/section/",
              "/author", "/authors", "/about", "/subscribe",
              "/newsletter", "/podcast", "/video", "/videos",
              "/gallery", "/photo", "/live", "/shows"
          ]

          ARTICLE_PATTERNS = [
              r"/\d{4}/\d{2}/\d{2}/",
              r"/\d{4}/\d{2}/",
              r"-\d{6,}"
          ]

          def fetch_html(url):
              req = urllib.request.Request(url, headers=HEADERS)
              with urllib.request.urlopen(req, timeout=20) as r:
                  return r.read()

          def same_domain(href, domain):
              if href.startswith("/"):
                  return True
              netloc = urlparse(href).netloc
              return netloc == "" or netloc == domain or netloc.endswith("." + domain)

          def valid_headline(text):
              if not text:
                  return False
              text = text.strip()
              if len(text) < 30 or len(text) > 140:
                  return False
              if text.isupper():
                  return False
              if len(text.split()) < 5:
                  return False
              return True

          def looks_like_article(url):
              u = url.lower()
              for bad in BAD_KEYWORDS:
                  if bad in u:
                      return False
              for p in ARTICLE_PATTERNS:
                  if re.search(p, u):
                      return True
              slug = u.rstrip("/").split("/")[-1]
              return slug.count("-") >= 3 and len(slug) > 25

          def get_meta(url):
              try:
                  req = urllib.request.Request(url, headers=HEADERS)
                  with urllib.request.urlopen(req, timeout=12) as r:
                      soup = BeautifulSoup(r.read(), "html.parser")

                  img = soup.find("meta", property="og:image")
                  image = img["content"] if img and img.get("content") else None

                  published = None
                  time_tag = soup.find("time")
                  if time_tag and time_tag.get("datetime"):
                      published = time_tag["datetime"]

                  summary = None
                  for p in soup.find_all("p"):
                      t = p.get_text(strip=True)
                      if t and len(t) > 60:
                          summary = t
                          break

                  return image, published, summary
              except Exception:
                  return None, None, None

          def scrape_source(src, limit=25):
              soup = BeautifulSoup(fetch_html(src["url"]), "html.parser")
              results = []
              seen = set()

              for a in soup.select("article a[href], h2 a[href], h3 a[href]"):
                  href = a.get("href")
                  title = a.get_text(strip=True)

                  if not href or not valid_headline(title):
                      continue
                  if not same_domain(href, src["domain"]):
                      continue

                  url = urljoin(src["url"], href)
                  if url in seen or not looks_like_article(url):
                      continue

                  image, published, summary = get_meta(url)
                  seen.add(url)

                  results.append({
                      "headline": title,
                      "url": url,
                      "published_at": published,
                      "summary": summary,
                      "image_url": image
                  })

                  if len(results) >= limit:
                      break

              return results

          all_items = []
          seen_urls = set()

          for s in SOURCES:
              try:
                  for item in scrape_source(s):
                      if item["url"] not in seen_urls:
                          seen_urls.add(item["url"])
                          all_items.append(item)
              except Exception as e:
                  print("Failed:", s["url"], e)

          existing = []
          if Path("us_news.json").exists():
              existing = json.loads(Path("us_news.json").read_text()).get("stories", [])

          def dedupe(items):
              out, seen = [], set()
              for i in items:
                  u = i.get("url")
                  if u and u not in seen:
                      seen.add(u)
                      out.append(i)
              return out

          final = dedupe(all_items + existing)

          with open("us_news.json", "w", encoding="utf-8") as f:
              json.dump(
                  {
                      "fetched_at": datetime.now(timezone.utc).isoformat(),
                      "stories": final
                  },
                  f,
                  ensure_ascii=False,
                  indent=2
              )

          print("Saved", len(final), "clean articles")
          PY

      - name: Commit and push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add us_news.json
          if git diff --cached --quiet; then
            echo "No changes"
            exit 0
          fi
          git commit -m "chore: update clean US news"
          git push
