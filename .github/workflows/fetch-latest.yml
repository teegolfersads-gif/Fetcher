name: Fetch US News (Google Full Coverage)

on:
  schedule:
    - cron: "*/30 * * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4

      - name: Fetch US news
        run: |
          python <<'PY'
          import json
          import urllib.request
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlparse
          from pathlib import Path
          from datetime import datetime, timezone

          HEADERS = {
              "User-Agent": "Mozilla/5.0"
          }

          GOOGLE_TOPICS = [
              "https://news.google.com/topics/CAAqIggKIhxDQkFTRHdvSkwyMHZNRGxqTjNjd0VnSmxiaWdBUAE?hl=en-US&gl=US&ceid=US:en",
              "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US:en"
          ]

          def fetch(url):
              req = urllib.request.Request(url, headers=HEADERS)
              with urllib.request.urlopen(req, timeout=20) as r:
                  return r.read()

          def full_coverage_links(topic_url):
              soup = BeautifulSoup(fetch(topic_url), "html.parser")
              links = set()
              for a in soup.select("a"):
                  if a.get_text(strip=True).lower() == "full coverage":
                      links.add(urljoin("https://news.google.com/", a.get("href")))
              return links

          def articles_from_fc(fc_url):
              soup = BeautifulSoup(fetch(fc_url), "html.parser")
              items = []
              seen = set()
              for a in soup.select("a[href]"):
                  title = a.get_text(strip=True)
                  href = a.get("href")
                  if not title or len(title) < 25:
                      continue
                  url = urljoin("https://news.google.com/", href)
                  if "news.google.com" in url:
                      continue
                  if url in seen:
                      continue
                  seen.add(url)
                  items.append({
                      "headline": title,
                      "url": url
                  })
              return items

          stories = []
          seen_urls = set()

          for topic in GOOGLE_TOPICS:
              for fc in full_coverage_links(topic):
                  for item in articles_from_fc(fc):
                      if item["url"] not in seen_urls:
                          seen_urls.add(item["url"])
                          stories.append(item)

          Path("us_news.json").write_text(
              json.dumps(
                  {
                      "fetched_at": datetime.now(timezone.utc).isoformat(),
                      "stories": stories
                  },
                  indent=2
              ),
              encoding="utf-8"
          )

          print("Saved", len(stories), "articles")
          PY

      - name: Commit and push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add us_news.json
          git diff --cached --quiet || git commit -m "chore: update US news"
          git push
