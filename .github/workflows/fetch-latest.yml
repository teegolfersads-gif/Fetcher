      - name: Fetch US news (Google News Full Coverage only)
        run: |
          python - <<'PY'
          import json
          import urllib.request
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlparse
          from pathlib import Path
          from datetime import datetime, timezone

          HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
          }

          GOOGLE_TOPICS = [
              "https://news.google.com/topics/CAAqIggKIhxDQkFTRHdvSkwyMHZNRGxqTjNjd0VnSmxiaWdBUAE?hl=en-US&gl=US&ceid=US:en",
              "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US:en"
          ]

          def fetch_html(url):
              req = urllib.request.Request(url, headers=HEADERS)
              with urllib.request.urlopen(req, timeout=20) as r:
                  return r.read()

          def extract_full_coverage_links(topic_url):
              soup = BeautifulSoup(fetch_html(topic_url), "html.parser")
              links = set()

              for a in soup.select("a"):
                  text = a.get_text(strip=True).lower()
                  href = a.get("href")

                  if text == "full coverage" and href:
                      full_url = urljoin("https://news.google.com/", href)
                      links.add(full_url)

              return list(links)

          def extract_articles_from_full_coverage(fc_url, limit=20):
              soup = BeautifulSoup(fetch_html(fc_url), "html.parser")
              results = []
              seen = set()

              for a in soup.select("a[href]"):
                  href = a.get("href")
                  title = a.get_text(strip=True)

                  if not href or not title or len(title) < 25:
                      continue

                  url = urljoin("https://news.google.com/", href)
                  parsed = urlparse(url)

                  # Skip Google internal links
                  if "news.google.com" in parsed.netloc:
                      continue

                  if url in seen:
                      continue

                  seen.add(url)
                  results.append({
                      "headline": title,
                      "url": url,
                      "source": parsed.netloc
                  })

                  if len(results) >= limit:
                      break

              return results

          all_articles = []
          seen_urls = set()

          for topic in GOOGLE_TOPICS:
              try:
                  full_pages = extract_full_coverage_links(topic)
                  for fc in full_pages:
                      for item in extract_articles_from_full_coverage(fc):
                          if item["url"] not in seen_urls:
                              seen_urls.add(item["url"])
                              all_articles.append(item)
              except Exception as e:
                  print("Failed topic:", topic, e)

          existing = []
          if Path("us_news.json").exists():
              existing = json.loads(Path("us_news.json").read_text()).get("stories", [])

          def dedupe(items):
              out, seen = [], set()
              for i in items:
                  u = i.get("url")
                  if u and u not in seen:
                      seen.add(u)
                      out.append(i)
              return out

          final = dedupe(all_articles + existing)

          with open("us_news.json", "w", encoding="utf-8") as f:
              json.dump(
                  {
                      "fetched_at": datetime.now(timezone.utc).isoformat(),
                      "stories": final
                  },
                  f,
                  ensure_ascii=False,
                  indent=2
              )

          print("Saved", len(final), "articles from Full Coverage pages")
          PY
